{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulated-footwear",
   "metadata": {},
   "source": [
    "## Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "outstanding-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Done\n"
     ]
    }
   ],
   "source": [
    "# codes are from https://github.com/tamlhp/template/blob/master/dblp/download_paperinfo_from_dblp.ipynb\n",
    "# with lil bit of revision\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "import pdb\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def site_open(site):\n",
    "    '''Makes connection and opens up target website. Returns a website object.'''\n",
    "    try:\n",
    "        #sets up request object\n",
    "        req = urllib.request.Request(site)\n",
    "\n",
    "        #adds User-Agent info to request object\n",
    "        req.add_header(\"User-Agent\",\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) \\\n",
    "         AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.46 Safari/536.5\")\n",
    "\n",
    "        #opens up site\n",
    "        website = urllib.request.urlopen(req)\n",
    "\n",
    "        return website\n",
    "    except urllib.request.URLError:\n",
    "        print('Could not connect to '+ site + '!')\n",
    "        pass\n",
    "    \n",
    "def soup_site(site):\n",
    "    '''opens site and turns it into a format to easily parse the DOM. Returns a Soup Object'''\n",
    "    try:\n",
    "        website = site_open(site)\n",
    "#         return BeautifulSoup(website, \"html5lib\")\n",
    "        return BeautifulSoup(website, \"lxml\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1\n",
    "\n",
    "print(\"Initialization Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "desirable-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ricardo Baeza-Yates'] Ethical Challenges in AI. 14th WSDM 2022:\n",
      "Virtual Event / Tempe, AZ, USA https://doi.org/10.1145/3488560.3498370\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# conf_name = 'cscw'\n",
    "conf_name = 'wsdm'\n",
    "\n",
    "year = 2022\n",
    "#year = 2021\n",
    "\n",
    "#soup = soup_site(\"https://dblp.org/db/conf/cscw/cscw{}c.html\".format(year))#CSCW\n",
    "soup = soup_site(\"https://dblp.org/db/conf/wsdm/wsdm{}.html\".format(year))#WSDM\n",
    "    \n",
    "if soup != -1:\n",
    "    title = soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"cite\", {\"class\" : \"data tts-content\"}).find(\"span\", {\"class\" : \"title\"}).text\n",
    "    authors = [t.text for t in soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"cite\", {\"class\" : \"data tts-content\"}).findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "    year = soup.find(\"header\", {\"id\": \"headline\"}).find(\"h1\").text\n",
    "    doi = soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "    print(authors, title, year, doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "automotive-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#conf_name = 'cscw'\n",
    "#conf_name = 'wsdm'\n",
    "#conf_name = 'chi'\n",
    "#conf_name = 'iswc'\n",
    "conf_name = 'ubicomp'\n",
    "\n",
    "#year = 2022\n",
    "#year = 2021\n",
    "year = 2020\n",
    "\n",
    "#volume = \"https://dblp.org/db/conf/wsdm/wsdm{}.html\".format(year)\n",
    "#volume = \"https://dblp.org/db/conf/cscw/cscw{}c.html\".format(year)#CSCW\n",
    "#volume = \"https://dblp.org/db/conf/chi/chi{}.html\".format(year)#CHI\n",
    "#volume = \"https://dblp.org/db/conf/iswc/iswc{}.html\".format(year)#ISWC\n",
    "#volume = \"https://dblp.org/db/conf/huc/ubicomp{}ap.html\".format(year)#UbiComp2021\n",
    "#volume = \"https://dblp.org/db/conf/iswc/iswc{}.html\".format(year)#ISWC2020\n",
    "volume = \"https://dblp.org/db/conf/huc/ubicomp{}ap.html\".format(year)#UbiComp2020\n",
    "\n",
    "f = open('crawled_csv_files/{}{}.csv'.format(conf_name,year), 'w', newline='')\n",
    "\n",
    "writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "soup = soup_site(volume)\n",
    "\n",
    "if soup != -1:\n",
    "    year = re.findall(r'\\d+', soup.find(\"header\", {\"id\": \"headline\"}).find(\"h1\").text)[0]\n",
    "\n",
    "    for paper in soup.findAll(\"li\", {\"class\" : \"entry inproceedings\"}):\n",
    "        try:\n",
    "            doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "        except:\n",
    "            doi = \"N/A\"\n",
    "\n",
    "        data = paper.find(\"cite\", {\"class\" : \"data tts-content\"})\n",
    "        if data is None:\n",
    "            data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "        if data is None:\n",
    "            data = paper.find(\"cite\", {\"class\" : \"data\"})\n",
    "        authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "        #authors = ','.join(authors)\n",
    "        title = data.find(\"span\", {\"class\" : \"title\"}).text\n",
    "        writer.writerow([\", \".join(authors), title, year, doi])\n",
    "\n",
    "    f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-brush",
   "metadata": {},
   "source": [
    "## Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "meaningful-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#journal_name = 'snam'\n",
    "#journal_name = 'tmm'\n",
    "#journal_name = 'pami'\n",
    "\n",
    "#year = 2023\n",
    "#year = 2022\n",
    "year = 2021\n",
    "\n",
    "# volume = \"https://dblp.org/db/journals/snam/snam12.html\"#SNAM2022\n",
    "# volume = \"https://dblp.org/db/journals/snam/snam11.html\"#SNAM2021\n",
    "# volume = \"https://dblp.org/db/journals/tmm/tmm24.html\"#TMM2022\n",
    "# volume = \"https://dblp.org/db/journals/tmm/tmm23.html\"#TMM2021\n",
    "# volume = \"https://dblp.org/db/journals/pami/pami45.html\"#PAMI2023\n",
    "# volume = \"https://dblp.org/db/journals/pami/pami44.html\"#PAMI2022\n",
    "\n",
    "\n",
    "f = open('crawled_csv_files/{}{}.csv'.format(journal_name,year), 'w', newline='')\n",
    "\n",
    "writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    \n",
    "soup = soup_site(volume)\n",
    "\n",
    "if soup != -1:\n",
    "    h2sections = soup.findAll(\"h2\")\n",
    "    year = 0\n",
    "    for h2section in h2sections:\n",
    "        match = re.match(r'.*([1-3][0-9]{3})', h2section.text)\n",
    "        if match is not None:\n",
    "            year = match.group(1)\n",
    "            break  \n",
    "    print(year)\n",
    "\n",
    "    for j, section in enumerate(soup.findAll(\"ul\", {\"class\" : \"publ-list\"})):\n",
    "        for paper in section.findAll(\"li\", {\"class\" : \"entry article\"}):\n",
    "            try:\n",
    "                doi = paper.find(\"nav\", {\"class\" : \"publ\"}).find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "            except:\n",
    "                doi = \"error\"\n",
    "            data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "            if data is None:\n",
    "                data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "            if data is None:\n",
    "                data = paper.find(\"cite\", {\"class\" : \"data\"})\n",
    "            authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "            title = data.find(\"span\", {\"class\" : \"title\"}).text.encode(\"utf-8\")\n",
    "            writer.writerow([\", \".join(authors).encode(\"utf-8\"), title, year, doi])\n",
    "\n",
    "    f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-attendance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
